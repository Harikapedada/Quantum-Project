# -*- coding: utf-8 -*-
"""CNN_conclude.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15csaySeWMDZLnLzQwowqycvaGMIieRWV
"""

!pip install pandas numpy scikit-learn tensorflow matplotlib seaborn imbalanced-learn

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping

df = pd.read_csv("/content/creditcard.csv")  # Make sure CSV path is correct

df['Hour'] = (df['Time'] // 3600) % 24
df['Log_Amount'] = np.log1p(df['Amount'])
df.drop(columns=['Time', 'Amount'], inplace=True)

# Separate fraud and legitimate transactions
df_fraud = df[df['Class'] == 1]
df_legit = df[df['Class'] == 0]

# Undersample majority class (~5:1 ratio)
df_legit_sampled = df_legit.sample(n=len(df_fraud)*5, random_state=42)
df_balanced = pd.concat([df_fraud, df_legit_sampled]).sample(frac=1, random_state=42)

# Separate features and labels
X = df_balanced.drop(columns=['Class'])
y = df_balanced['Class']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# âœ… Add this to check training data size
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

# Reshape for 1D CNN
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

input_layer = tf.keras.Input(shape=(X_train.shape[1], 1))
x = tf.keras.layers.Conv1D(filters=32, kernel_size=2, activation="relu")(input_layer)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation="relu")(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(64, activation="relu")(x)
x = tf.keras.layers.Dropout(0.5)(x)
output_layer = tf.keras.layers.Dense(1, activation="sigmoid")(x)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),
    loss="binary_crossentropy",
    metrics=["accuracy", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))
print("Class weights:", class_weight_dict)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=128,
    validation_data=(X_test, y_test),
    class_weight=class_weight_dict,

)

# Predict on test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluate test set metrics (optional)
print(f"Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print(f"Test Precision: {precision_score(y_test, y_pred) * 100:.2f}%")
print(f"Test Recall: {recall_score(y_test, y_pred) * 100:.2f}%")
print(f"Test F1 Score: {f1_score(y_test, y_pred) * 100:.2f}%")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Extract per-epoch metrics from training
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_recall = history.history['recall_3']  # Use the correct key
val_recall = history.history['val_recall_3'] # Use the correct key

# Print all metrics per epoch
print(f"{'Epoch':<6} {'Train Acc (%)':<15} {'Val Acc (%)':<15} {'Train Loss':<12} {'Val Loss':<12} {'Train Recall (%)':<18} {'Val Recall (%)':<18}")
for i in range(len(train_acc)):
    print(f"{i+1:<6} "
          f"{train_acc[i]*100:<15.2f} "
          f"{val_acc[i]*100:<15.2f} "
          f"{train_loss[i]:<12.4f} "
          f"{val_loss[i]:<12.4f} "
          f"{train_recall[i]*100:<18.2f} "
          f"{val_recall[i]*100:<18.2f}")

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()