#CNN_Fraud_Detection


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.callbacks import EarlyStopping

df = pd.read_csv("/content/creditcard.csv")  # Make sure CSV path is correct

df['Hour'] = (df['Time'] // 3600) % 24
df['Log_Amount'] = np.log1p(df['Amount'])
df.drop(columns=['Time', 'Amount'], inplace=True)

# Separate fraud and legitimate transactions
df_fraud = df[df['Class'] == 1]
df_legit = df[df['Class'] == 0]

# Undersample majority class (~5:1 ratio)
df_legit_sampled = df_legit.sample(n=len(df_fraud)*5, random_state=42)
df_balanced = pd.concat([df_fraud, df_legit_sampled]).sample(frac=1, random_state=42)

# Separate features and labels
X = df_balanced.drop(columns=['Class'])
y = df_balanced['Class']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ‚úÖ Add this to check training data size
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)

# Reshape for 1D CNN
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

input_layer = tf.keras.Input(shape=(X_train.shape[1], 1))
x = tf.keras.layers.Conv1D(filters=32, kernel_size=2, activation="relu")(input_layer)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Conv1D(filters=64, kernel_size=2, activation="relu")(x)
x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(64, activation="relu")(x)
x = tf.keras.layers.Dropout(0.5)(x)
output_layer = tf.keras.layers.Dense(1, activation="sigmoid")(x)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),
    loss="binary_crossentropy",
    metrics=["accuracy", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weight_dict = dict(enumerate(class_weights))
print("Class weights:", class_weight_dict)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_test, y_test),
    class_weight=class_weight_dict,

)

# Predict on test set
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluate test set metrics (optional)
print(f"Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print(f"Test Precision: {precision_score(y_test, y_pred) * 100:.2f}%")
print(f"Test Recall: {recall_score(y_test, y_pred) * 100:.2f}%")
print(f"Test F1 Score: {f1_score(y_test, y_pred) * 100:.2f}%")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Extract per-epoch metrics from training
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Find the correct key for recall
recall_key = None
for key in history.history.keys():
    if 'recall' in key:
        recall_key = key
        break

if recall_key:
    train_recall = history.history[recall_key]
    val_recall = history.history[f'val_{recall_key}']

    # Print all metrics per epoch
    print(f"{'Epoch':<6} {'Train Acc (%)':<15} {'Val Acc (%)':<15} {'Train Loss':<12} {'Val Loss':<12} {'Train Recall (%)':<18} {'Val Recall (%)':<18}")
    for i in range(len(train_acc)):
        print(f"{i+1:<6} "
              f"{train_acc[i]*100:<15.2f} "
              f"{val_acc[i]*100:<15.2f} "
              f"{train_loss[i]:<12.4f} "
              f"{val_loss[i]:<12.4f} "
              f"{train_recall[i]*100:<18.2f} "
              f"{val_recall[i]*100:<18.2f}")
else:
    print("Could not find recall key in history.")

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()



# =========================
# Gradio UI for CNN_fraud_detection
# =========================
import gradio as gr

def predict_from_csv(file):
    # Load uploaded CSV
    data = pd.read_csv(file.name)

    # Apply same preprocessing as training
    if "Time" in data.columns:
        data["Hour"] = (data["Time"] // 3600) % 24
        data["Log_Amount"] = np.log1p(data["Amount"])
        data = data.drop(columns=["Time", "Amount"])

    # Separate labels if present
    if "Class" in data.columns:
        y_true = data["Class"]
        X_data = data.drop(columns=["Class"])
    else:
        y_true = None
        X_data = data

    # Scale + reshape
    X_scaled = scaler.transform(X_data)
    X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))

    # Predict
    y_pred_prob = model.predict(X_scaled)
    y_pred = (y_pred_prob > 0.5).astype(int)

    # Results table
    results = pd.DataFrame({
        "Fraud_Probability": y_pred_prob.flatten(),
        "Prediction": ["Fraud" if p==1 else "Legit" for p in y_pred.flatten()]
    })

    # Compute metrics if labels exist
    metrics_text = "No ground truth labels found."
    if y_true is not None:
        acc = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred)
        rec = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        metrics_text = (f"Accuracy: {acc*100:.2f}%\n"
                        f"Precision: {prec*100:.2f}%\n"
                        f"Recall: {rec*100:.2f}%\n"
                        f"F1 Score: {f1*100:.2f}%")

    return results, metrics_text

# Build Gradio UI
demo = gr.Interface(
    fn=predict_from_csv,
    inputs=gr.File(label="Upload Credit Card CSV Dataset", file_types=[".csv"]),
    outputs=[
        gr.Dataframe(headers=["Fraud_Probability", "Prediction"], label="Prediction Results"),
        gr.Textbox(label="Evaluation Metrics")
    ],
    title="üí≥ Credit Card Fraud Detection",
    description="Upload a CSV dataset to detect Fraudulent vs Legitimate transactions."
)

# Run the app
demo.launch()



# QCNN_fraud_detection.py

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
from sklearn.impute import SimpleImputer
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
import tensorflow as tf
import pennylane as qml
import matplotlib.pyplot as plt

df = pd.read_csv("/content/creditcard.csv")

# Features and labels
X = df.drop(columns=["Time", "Class"])
y = df["Class"]

# Convert columns to numeric, coercing errors to NaN
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

# Handle missing values
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Standard scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# PCA to 8 components for 8 qubits
pca = PCA(n_components=8) # Keep 28 components for now as defined later in the notebook
X_pca = pca.fit_transform(X_scaled)
print(f"PCA Explained Variance: {pca.explained_variance_ratio_}")

X_cleaned = X_pca[~np.isnan(y)]
y_cleaned = y[~np.isnan(y)]

# SMOTE for class balancing
smote = SMOTE(random_state=42, k_neighbors=2)
X_balanced, y_balanced = smote.fit_resample(X_cleaned, y_cleaned)
print(f"Balanced class distribution: {dict(zip(*np.unique(y_balanced, return_counts=True)))}")

X_train, X_test, y_train, y_test = train_test_split(
    X_balanced, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42
)

y_train = y_train.to_numpy().flatten()
y_test = y_test.to_numpy().flatten()

import pennylane as qml
import tensorflow as tf

n_qubits = 28
dev = qml.device("default.qubit", wires=n_qubits)

# QNode
def quantum_circuit(inputs, weights_conv, weights_pool):
    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))
    qml.templates.StronglyEntanglingLayers(weights_conv, wires=range(n_qubits))

    # Quantum pooling example
    for i in range(0, n_qubits, 2):
        qml.CNOT(wires=[i, i+1])
        qml.RY(weights_pool[i//2], wires=i)

    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits//2)]

weight_shapes = {
    "weights_conv": (2, n_qubits, 3),
    "weights_pool": (n_qubits//2,)
}

# Wrap QNode into a Keras Layer
qnode = qml.QNode(quantum_circuit, dev, interface="tf")

class QuantumLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.qnode = qnode
        # Trainable weights
        self.weights_conv = tf.Variable(tf.random.normal(shape=weight_shapes["weights_conv"]), trainable=True)
        self.weights_pool = tf.Variable(tf.random.normal(shape=weight_shapes["weights_pool"]), trainable=True)

    def call(self, inputs):
        return self.qnode(inputs, self.weights_conv, self.weights_pool)

class QuantumLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.qnode = qnode
        self.weights_conv = tf.Variable(tf.random.normal(shape=weight_shapes["weights_conv"]), trainable=True)
        self.weights_pool = tf.Variable(tf.random.normal(shape=weight_shapes["weights_pool"]), trainable=True)

    def call(self, inputs):
        outputs = self.qnode(inputs, self.weights_conv, self.weights_pool)  # List of expectation values
        # Convert list to tensor of shape (batch_size, n_qubits//2)
        return tf.stack(outputs, axis=1)

model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Ensure input shape is correct (12)
    tf.keras.layers.Dense(64, activation='relu'),  # Temporarily replace quantum layer with a dense layer
    tf.keras.layers.Dense(1, activation="sigmoid")
])

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss="binary_crossentropy",
    metrics=["accuracy", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
)

history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,      # Colab-friendly
    batch_size=128
)

y_pred_probs = model.predict(X_test)
y_pred_classes = (y_pred_probs > 0.5).astype(int).flatten()

test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)
f1 = f1_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes)
recall = recall_score(y_test, y_pred_classes)
conf_matrix = confusion_matrix(y_test, y_pred_classes)

print(f"\n‚úÖ Test Loss: {test_loss:.4f}")
print(f"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"‚úÖ Test Precision: {precision * 100:.2f}%")
print(f"‚úÖ Test Recall: {recall * 100:.2f}%")
print(f"‚úÖ Test F1 Score: {f1 * 100:.2f}%")
print(f"\nConfusion Matrix:\n{conf_matrix}")

# Extract and print training and validation metrics
print(f"\nTraining Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"Training Loss: {history.history['loss'][-1]:.4f}")
print(f"Validation Loss: {history.history['val_loss'][-1]:.4f}")
print(f"Training Recall: {history.history['recall'][-1] * 100:.2f}%")
print(f"Validation Recall: {history.history['val_recall'][-1] * 100:.2f}%")

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title("Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

# QCNN_Fraud_detection

import gradio as gr
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import tempfile

# --- Assume model, imputer, scaler, pca are already trained and available ---

def predict_from_csv(file):
    # Load uploaded CSV
    df = pd.read_csv(file)

    # Drop unused columns
    if "Time" in df.columns:
        df = df.drop(columns=["Time"])
    y = df["Class"] if "Class" in df.columns else None
    X = df.drop(columns=["Class"], errors="ignore")

    # Convert to numeric
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors="coerce")

    # Preprocessing
    X_imputed = imputer.transform(X)
    X_scaled = scaler.transform(X_imputed)
    X_pca = pca.transform(X_scaled)

    # Predictions
    probs = model.predict(X_pca).flatten()
    preds = (probs > 0.5).astype(int)

    # Build full output DataFrame
    result = df.copy()
    result["Fraud Probability"] = probs
    result["Prediction"] = preds

    # Compute metrics if labels exist
    if y is not None:
        acc = accuracy_score(y, preds)
        prec = precision_score(y, preds, zero_division=0)
        rec = recall_score(y, preds, zero_division=0)
        f1 = f1_score(y, preds, zero_division=0)
        cm = confusion_matrix(y, preds)

        metrics_text = (
            f"‚úÖ Accuracy: {acc*100:.2f}%\n"
            f"‚úÖ Precision: {prec*100:.2f}%\n"
            f"‚úÖ Recall: {rec*100:.2f}%\n"
            f"‚úÖ F1 Score: {f1*100:.2f}%\n"
            f"\nConfusion Matrix:\n{cm}"
        )
    else:
        metrics_text = "‚ö†Ô∏è No 'Class' column found in the CSV, so accuracy cannot be computed."

    # Save full results to a temporary CSV for download
    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".csv")
    result.to_csv(tmp_file.name, index=False)

    return result, metrics_text, tmp_file.name


# Gradio UI
demo = gr.Interface(
    fn=predict_from_csv,
    inputs=gr.File(label="Upload CSV", file_types=[".csv"]),
    outputs=[
        gr.Dataframe(label="Fraud Detection Results (entire dataset)"),
        gr.Textbox(label="Evaluation Metrics", lines=10),  # bigger box
        gr.File(label="Download Full Results as CSV")
    ],
    title="üí≥ Fraud Detection QCNN",
    description="Upload a CSV file (like creditcard.csv). The model predicts fraud probability for each row. If 'Class' column is present, accuracy and other metrics are shown."
)

demo.launch()


